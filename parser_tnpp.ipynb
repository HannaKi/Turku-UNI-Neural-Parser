{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parser-tnpp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fginter/ainl_2020_tutorial/blob/main/parser_tnpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCRShOw-W9Fk"
      },
      "source": [
        "# Turku Neural Parser Pipeline - Python module version\n",
        "\n",
        "* This is a basic tutorial for running the parser pipeline under Google Colab\n",
        "* Many new properties added for the AINL2020 tutorial\n",
        "  * Parser changed to Udify (87->91% LAS improvement)\n",
        "  * Code restructured so as to be importable as a module and can be installed using pip\n",
        "  * No longer depends on Tensorflow\n",
        "* Makes it possible for anyone to run the parser with GPU acceleration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byCtKqKGX-Yy"
      },
      "source": [
        "# Install\n",
        "\n",
        "* Pre-built wheel, not yet in PyPi\n",
        "* Only Finnish and English models in this tutorial, more coming!\n",
        "\n",
        "## Install the parser package (takes its time)\n",
        "\n",
        "`pip3 install http://dl.turkunlp.org/turku-parser-models/turku_neural_parser-0.2-py3-none-any.whl`\n",
        "\n",
        "## Download and unpack the model\n",
        "\n",
        "`wget http://dl.turkunlp.org/turku-parser-models/models_fi_tdt.tar.gz ; tar zxvf models_fi_tdt.tar.gz`\n",
        "\n",
        "...and you are good to go!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3zn4zrJwAEB"
      },
      "source": [
        "#Gdown is a utility for downloading large files from Google Drive, where I mirrored the NER trained model for you\n",
        "\n",
        "!pip3 install gdown\n",
        "!gdown -O turku_neural_parser-0.2-py3-none-any.whl 'https://drive.google.com/uc?export=download&id=1m8Nhd1oU6eS559D0Xboe83NbTzcMsyjN'\n",
        "\n",
        "!pip3 install turku_neural_parser-0.2-py3-none-any.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue9vzD93xLyt",
        "outputId": "dda16018-644a-4217-d078-bf5940b71df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "!gdown -O models_fi_tdt.tar.gz 'https://drive.google.com/uc?export=download&id=157r-qRi0YxuN82U252I4RHOXDPv-PgXg'\n",
        "!tar zxvf models_fi_tdt.tar.gz"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=157r-qRi0YxuN82U252I4RHOXDPv-PgXg\n",
            "To: /content/models_fi_tdt.tar.gz\n",
            "1.11GB [00:18, 61.2MB/s]\n",
            "models_fi_tdt/\n",
            "models_fi_tdt/Udify/\n",
            "models_fi_tdt/Udify/tdt-udify-model.tar.gz\n",
            "models_fi_tdt/Udify/weights.th\n",
            "models_fi_tdt/Udify/vocabulary/\n",
            "models_fi_tdt/Udify/vocabulary/head_tags.txt\n",
            "models_fi_tdt/Udify/vocabulary/feats.txt\n",
            "models_fi_tdt/Udify/vocabulary/token_characters.txt\n",
            "models_fi_tdt/Udify/vocabulary/tokens.txt\n",
            "models_fi_tdt/Udify/vocabulary/upos.txt\n",
            "models_fi_tdt/Udify/vocabulary/xpos.txt\n",
            "models_fi_tdt/Udify/vocabulary/non_padded_namespaces.txt\n",
            "models_fi_tdt/Udify/vocabulary/lemmas.txt\n",
            "models_fi_tdt/Udify/config.json\n",
            "models_fi_tdt/pipelines.yaml\n",
            "models_fi_tdt/README\n",
            "models_fi_tdt/Lemmatizer/\n",
            "models_fi_tdt/Lemmatizer/lemmatizer.pt\n",
            "models_fi_tdt/Tokenizer/\n",
            "models_fi_tdt/Tokenizer/tokenizer.udpipe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpRs20VfbLT2"
      },
      "source": [
        "# Running the parser\n",
        "\n",
        "* Every model can specify many processing pipelines\n",
        "* These are in `modeldir/pipelines.yaml`\n",
        "* `parse_plaintext`is the default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jCpvDULxW8p",
        "outputId": "69a439ce-9782-451b-b839-147542b5d999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from tnparser.pipeline import read_pipelines, Pipeline\n",
        "\n",
        "available_pipelines=read_pipelines(\"models_fi_tdt/pipelines.yaml\")\n",
        "print(list(available_pipelines.keys()))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['parse_plaintext', 'parse_sentlines', 'parse_wslines', 'parse_conllu', 'tokenize']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeFgUzS5dLKp",
        "outputId": "040b753e-1ff7-484e-f9ba-e7943968a074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "p=Pipeline(available_pipelines[\"parse_plaintext\"])\n",
        "parsed=p.parse(\"Minulla on ruskea koira! Se haukkuu ja juoksee. Voi ett√§!\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset reader: <class 'udify.dataset_readers.universal_dependencies.UniversalDependenciesDatasetReader'>\n",
            "0it [00:00, ?it/s]Your label namespace was 'upos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "Your label namespace was 'xpos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "Your label namespace was 'feats'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "Your label namespace was 'lemmas'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "3it [00:00, 140.41it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/allennlp/data/token_indexers/token_indexer.py:119: FutureWarning: Using a Field with pad_token_sequence, which will be depreciated in 1.0.0.Please implement as_padded_tensor instead.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/udify/models/dependency_decoder.py:493: UserWarning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorIterator.cpp:918.)\n",
            "  normalized_arc_logits.unsqueeze(1) + normalized_pairwise_head_logits\n",
            "Encountered the arc_loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "Encountered the tag_loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "Encountered the loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            " >>> 12/13 unique tokens submitted to lemmatizer\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:52: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: OrderedIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqZokOvhd7KW",
        "outputId": "fc5d6af9-13e2-4aa7-dac4-3a18f9c87074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "print(parsed)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\tMinulla\tmin√§\tPRON\t_\tCase=Ade|Number=Sing|Person=1|PronType=Prs\t0\troot\t_\t_\n",
            "2\ton\tolla\tAUX\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t1\tcop:own\t_\t_\n",
            "3\truskea\truskea\tADJ\t_\tCase=Nom|Degree=Pos|Number=Sing\t4\tamod\t_\t_\n",
            "4\tkoira\tkoira\tNOUN\t_\tCase=Nom|Number=Sing\t1\tnsubj:cop\t_\t_\n",
            "5\t!\t!\tPUNCT\t_\t_\t1\tpunct\t_\t_\n",
            "\n",
            "1\tSe\tse\tPRON\t_\tCase=Nom|Number=Sing|PronType=Dem\t2\tnsubj\t_\t_\n",
            "2\thaukkuu\thaukkua\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\n",
            "3\tja\tja\tCCONJ\t_\t_\t4\tcc\t_\t_\n",
            "4\tjuoksee\tjuosta\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t2\tconj\t_\t_\n",
            "5\t.\t.\tPUNCT\t_\t_\t2\tpunct\t_\t_\n",
            "\n",
            "1\tVoi\tvoi\tINTJ\t_\t_\t0\troot\t_\t_\n",
            "2\tett√§\tett√§\tINTJ\t_\t_\t1\tfixed\t_\t_\n",
            "3\t!\t!\tPUNCT\t_\t_\t1\tpunct\t_\t_\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bXEDdThfM-Z"
      },
      "source": [
        "# GPU mode\n",
        "\n",
        "* The pipeline runs by default in CPU mode\n",
        "* Needs to be told to run in GPU\n",
        "* This is a bit tricky right now but not impossible\n",
        "* Note: if you now switch the Runtime into GPU, you need to re-run the pip install\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt3slQjCfoEd",
        "outputId": "0c82b270-14b8-4bc7-85a7-94d237c7a0fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "#I do realize this ain't good! :)\n",
        "import types\n",
        "extra_args=types.SimpleNamespace()\n",
        "extra_args.__dict__[\"udify_mod.device\"]=\"0\" #simulates someone giving a --device 0 parameter to Udify\n",
        "extra_args.__dict__[\"lemmatizer_mod.device\"]=\"0\" \n",
        "\n",
        "p=Pipeline(available_pipelines[\"parse_plaintext\"],extra_args)\n",
        "parsed=p.parse(\"Minulla on ruskea koira! Se haukkuu ja juoksee. Voi ett√§!\")\n",
        "print(\"Parsed has this many lines:\",len(parsed.split(\"\\n\")))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset reader: <class 'udify.dataset_readers.universal_dependencies.UniversalDependenciesDatasetReader'>\n",
            "0it [00:00, ?it/s]Your label namespace was 'upos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "Your label namespace was 'xpos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "Your label namespace was 'feats'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "Your label namespace was 'lemmas'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "3it [00:00, 168.62it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/allennlp/data/token_indexers/token_indexer.py:119: FutureWarning: Using a Field with pad_token_sequence, which will be depreciated in 1.0.0.Please implement as_padded_tensor instead.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/udify/models/dependency_decoder.py:493: UserWarning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorIterator.cpp:918.)\n",
            "  normalized_arc_logits.unsqueeze(1) + normalized_pairwise_head_logits\n",
            "Encountered the arc_loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "Encountered the tag_loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "Encountered the loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            " >>> 12/13 unique tokens submitted to lemmatizer\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:52: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: OrderedIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Parsed has this many lines: 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRGxN0bN1Nev",
        "outputId": "518a0d0d-0311-4c59-bb52-f6f24b1d075c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#Since we are on a GPU, we can try to push through quite a bit more of data\n",
        "parsed=p.parse(\"Minulla on ruskea koira! Se haukkuu ja juoksee. Voi ett√§! \"*200) #takes forever on CPU, finishes in few seconds on GPU\n",
        "print(\"Parsed has this many lines:\",len(parsed.split(\"\\n\")))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsed has this many lines: 3201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G90KC9dFfp-h"
      },
      "source": [
        "# Process the output\n",
        "\n",
        "* The output of the pipeline run is a conll-u string\n",
        "* You can parse it in any number of ways\n",
        "* This is my preferred:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enQ-xvnff599",
        "outputId": "281c654b-54c7-4549-8f42-c8a91d1fcfa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "ID,FORM,LEMMA,UPOS,XPOS,FEAT,HEAD,DEPREL,DEPS,MISC=range(10) #the 10 columns\n",
        "\n",
        "def read_conll(inp,max_sent=0,drop_tokens=True,drop_nulls=True):\n",
        "    \"\"\"\n",
        "    inp: list of lines or an open file\n",
        "    max_sent: 0 for all, >0 to limit\n",
        "    drop_tokens: ignore multiword token lines\n",
        "    drop_nulls: ignore null nodes in enhanced dependencies\n",
        "\n",
        "    Yields lines of the parse and comments\n",
        "    \"\"\"\n",
        "\n",
        "    comments=[]\n",
        "    sent=[]\n",
        "    yielded=0\n",
        "    for line in inp:\n",
        "        line=line.rstrip(\"\\n\")\n",
        "        if line.startswith(\"#\"):\n",
        "            comments.append(line)\n",
        "        elif not line:\n",
        "            if sent:\n",
        "                yield sent,comments\n",
        "                yielded+=1\n",
        "                if max_sent>0 and yielded==max_sent:\n",
        "                    break\n",
        "                sent,comments=[],[]\n",
        "        else:\n",
        "            cols=line.split(\"\\t\")\n",
        "            if drop_tokens and \"-\" in cols[ID]:\n",
        "                continue\n",
        "            if drop_nulls and \".\" in cols[ID]:\n",
        "                continue\n",
        "            sent.append(cols)\n",
        "    else:\n",
        "        if sent:\n",
        "            yield sent,comments\n",
        "\n",
        "for one_sent,comments in read_conll(parsed.split(\"\\n\"),5):\n",
        "    words=(word_line[FORM] for word_line in one_sent)\n",
        "    lemmas=(word_line[LEMMA] for word_line in one_sent)\n",
        "    print(\" \".join(words))\n",
        "    print(\" \".join(lemmas))\n",
        "    print()\n",
        "\n",
        "# and that's really all there is to it :)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minulla on ruskea koira !\n",
            "min√§ olla ruskea koira !\n",
            "\n",
            "Se haukkuu ja juoksee .\n",
            "se haukkua ja juosta .\n",
            "\n",
            "Voi ett√§ !\n",
            "voi ett√§ !\n",
            "\n",
            "Minulla on ruskea koira !\n",
            "min√§ olla ruskea koira !\n",
            "\n",
            "Se haukkuu ja juoksee .\n",
            "se haukkua ja juosta .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}