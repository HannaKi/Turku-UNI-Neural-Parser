{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parser-tnpp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannaKi/Turku-UNI-Neural-Parser/blob/main/parser_tnpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCRShOw-W9Fk"
      },
      "source": [
        "# Turku Neural Parser Pipeline - Python module version\n",
        "\n",
        "* This is a basic tutorial for running the parser pipeline under Google Colab\n",
        "* Many new properties added for the AINL2020 tutorial\n",
        "  * Parser changed to Udify (87->91% LAS improvement)\n",
        "  * Code restructured so as to be importable as a module and can be installed using pip\n",
        "  * No longer depends on Tensorflow\n",
        "* Makes it possible for anyone to run the parser with GPU acceleration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byCtKqKGX-Yy"
      },
      "source": [
        "# Install\n",
        "\n",
        "* Pre-built wheel, not yet in PyPi\n",
        "* Only Finnish and English models in this tutorial, more coming!\n",
        "\n",
        "## Install the parser package (takes its time)\n",
        "\n",
        "`pip3 install http://dl.turkunlp.org/turku-parser-models/turku_neural_parser-0.3-py3-none-any.whl`\n",
        "\n",
        "## Download and unpack the model\n",
        "\n",
        "`wget http://dl.turkunlp.org/turku-parser-models/models_fi_tdt_v2.7.tar.gz ; tar zxvf models_fi_tdt.tar.gz`\n",
        "\n",
        "...and you are good to go!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM38KujEb2dT"
      },
      "source": [
        "# Prerequisites:\n",
        "\n",
        "* The models here are tested with torch 1.7\n",
        "* It might be that at some point this notebook will break\n",
        "* If that happens, try to install torch 1.7\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_klczVZaSpJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dbd4010-58d1-4dc5-df28-f53a2358ddf6"
      },
      "source": [
        "!nvcc --version\n",
        "!python -V"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3zn4zrJwAEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c36f0e83-2edd-43f0-943d-e3b1386ba8ff"
      },
      "source": [
        "!wget -nc http://dl.turkunlp.org/turku-parser-models/turku_neural_parser-0.3-py3-none-any.whl\n",
        "!pip3 install turku_neural_parser-0.3-py3-none-any.whl"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-15 10:15:07--  http://dl.turkunlp.org/turku-parser-models/turku_neural_parser-0.3-py3-none-any.whl\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99697 (97K) [application/octet-stream]\n",
            "Saving to: ‘turku_neural_parser-0.3-py3-none-any.whl’\n",
            "\n",
            "turku_neural_parser 100%[===================>]  97.36K   114KB/s    in 0.9s    \n",
            "\n",
            "2020-12-15 10:15:08 (114 KB/s) - ‘turku_neural_parser-0.3-py3-none-any.whl’ saved [99697/99697]\n",
            "\n",
            "Processing ./turku_neural_parser-0.3-py3-none-any.whl\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from turku-neural-parser==0.3) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from turku-neural-parser==0.3) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from turku-neural-parser==0.3) (2.23.0)\n",
            "Collecting configargparse\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/79/3045743bb26ca2e44a1d317c37395462bfed82dbbd38e69a3280b63696ce/ConfigArgParse-1.2.3.tar.gz (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.8MB/s \n",
            "\u001b[?25hCollecting ufal.udpipe\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 6.4MB/s \n",
            "\u001b[?25hCollecting allennlp==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 8.9MB/s \n",
            "\u001b[?25hCollecting torchtext>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/81/be2d72b1ea641afc74557574650a5b421134198de9f68f483ab10d515dca/torchtext-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from turku-neural-parser==0.3) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from turku-neural-parser==0.3) (1.18.5)\n",
            "Collecting OpenNMT-py>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/20/40f8b722aa0e35e259c144b6ec2d684f1aea7de869cf586c67cfd6fe1c55/OpenNMT_py-1.2.0-py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 38.3MB/s \n",
            "\u001b[?25hCollecting transformers==2.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 41.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask->turku-neural-parser==0.3) (1.0.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask->turku-neural-parser==0.3) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->turku-neural-parser==0.3) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->turku-neural-parser==0.3) (2.11.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->turku-neural-parser==0.3) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->turku-neural-parser==0.3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->turku-neural-parser==0.3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->turku-neural-parser==0.3) (3.0.4)\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 40.5MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/cd/513fff674c22507caf5a983ac1aacf87fc207535ada17d720199b51b6cc3/boto3-1.16.36-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 42.3MB/s \n",
            "\u001b[?25hCollecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting spacy<2.2,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/5b/e07dd3bf104237bce4b398558b104c8e500333d6f30eabe3fa9685356b7d/spacy-2.1.9-cp36-cp36m-manylinux1_x86_64.whl (30.8MB)\n",
            "\u001b[K     |████████████████████████████████| 30.9MB 96kB/s \n",
            "\u001b[?25hCollecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/43/0e/2f50064e327f41a1eb811df089f813036e19a64b95e33f8e9e0b96c2447e/flaky-3.7.0-py2.py3-none-any.whl\n",
            "Collecting numpydoc>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1d/9e398c53d6ae27d5ab312ddc16a9ffe1bee0dfdf1d6ec88c40b0ca97582e/numpydoc-1.1.0-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.2MB/s \n",
            "\u001b[?25hCollecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/d5/1cc282dc23346a43aab461bf2e8c36593aacd34242bee1a13fa750db0cfe/jsonpickle-1.4.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0->turku-neural-parser==0.3) (3.2.2)\n",
            "Collecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25hCollecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/69/7f/d0aeaaafb5c3c76c8d2141dbe2d4f6dca5d6c31872d4e5349768c1958abc/Flask_Cors-3.0.9-py2.py3-none-any.whl\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.7MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 37.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0->turku-neural-parser==0.3) (3.2.5)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 38.3MB/s \n",
            "\u001b[?25hCollecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 40.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0->turku-neural-parser==0.3) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0->turku-neural-parser==0.3) (1.4.1)\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0->turku-neural-parser==0.3) (2018.9)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0->turku-neural-parser==0.3) (3.6.4)\n",
            "Collecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/71/4f04aed03ca35f2d02e1732ca6e996b2d7b40232fb7f1b58ff35f9a89b7b/responses-0.12.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0->turku-neural-parser==0.3) (0.22.2.post1)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0->turku-neural-parser==0.3) (0.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0->turku-neural-parser==0.3) (2.10.0)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0->turku-neural-parser==0.3) (0.5.3)\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 38.0MB/s \n",
            "\u001b[?25hCollecting gevent>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/92/b80b922f08f222faca53c8d278e2e612192bc74b0e1f0db2f80a6ee46982/gevent-20.9.0-cp36-cp36m-manylinux2010_x86_64.whl (5.3MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3MB 40.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->turku-neural-parser==0.3) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->turku-neural-parser==0.3) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->turku-neural-parser==0.3) (0.8)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py>=1.2.0->turku-neural-parser==0.3) (2.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py>=1.2.0->turku-neural-parser==0.3) (1.15.0)\n",
            "Collecting waitress\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/d1/5209fb8c764497a592363c47054436a515b47b8c3e4970ddd7184f088857/waitress-1.4.4-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n",
            "\u001b[?25hCollecting pyonmttok==1.*; platform_system == \"Linux\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/21/7a69fa68de7de41ef70b35424d21523ebf2208f0c0fab1355cabc2305ff4/pyonmttok-1.22.2-cp36-cp36m-manylinux1_x86_64.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 33.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->turku-neural-parser==0.3) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 36.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->turku-neural-parser==0.3) (2019.12.20)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 38.1MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 33.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->turku-neural-parser==0.3) (20.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->turku-neural-parser==0.3) (1.1.1)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.2MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.36\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/27/f8757c8d3d11a2332677e2be978f2a524ab13d07d3766e2fff18693e6f3d/botocore-1.19.36-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 38.9MB/s \n",
            "\u001b[?25hCollecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9.0->turku-neural-parser==0.3) (1.0.5)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9.0->turku-neural-parser==0.3) (0.8.0)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 19.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9.0->turku-neural-parser==0.3) (1.0.5)\n",
            "Collecting thinc<7.1.0,>=7.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 37.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9.0->turku-neural-parser==0.3) (2.0.5)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp==0.9.0->turku-neural-parser==0.3) (1.8.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp==0.9.0->turku-neural-parser==0.3) (3.1.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0->turku-neural-parser==0.3) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0->turku-neural-parser==0.3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0->turku-neural-parser==0.3) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0->turku-neural-parser==0.3) (2.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp==0.9.0->turku-neural-parser==0.3) (0.2.5)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9.0->turku-neural-parser==0.3) (20.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9.0->turku-neural-parser==0.3) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9.0->turku-neural-parser==0.3) (1.9.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9.0->turku-neural-parser==0.3) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9.0->turku-neural-parser==0.3) (8.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9.0->turku-neural-parser==0.3) (50.3.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp==0.9.0->turku-neural-parser==0.3) (0.17.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp==0.9.0->turku-neural-parser==0.3) (3.12.4)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/b0/da8afd9b3bd50c7665ecdac062f182982af1173c9081f9af7261091c5588/zope.interface-5.2.0-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 33.0MB/s \n",
            "\u001b[?25hCollecting greenlet>=0.4.17; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d0/532e160c777b42f6f393f9de8c88abb8af6c892037c55e4d3a8a211324dd/greenlet-0.4.17-cp36-cp36m-manylinux1_x86_64.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py>=1.2.0->turku-neural-parser==0.3) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py>=1.2.0->turku-neural-parser==0.3) (0.4.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py>=1.2.0->turku-neural-parser==0.3) (1.34.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py>=1.2.0->turku-neural-parser==0.3) (3.3.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py>=1.2.0->turku-neural-parser==0.3) (0.36.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py>=1.2.0->turku-neural-parser==0.3) (0.10.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py>=1.2.0->turku-neural-parser==0.3) (1.17.2)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0->turku-neural-parser==0.3) (1.2.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0->turku-neural-parser==0.3) (0.7.12)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0->turku-neural-parser==0.3) (2.6.1)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0->turku-neural-parser==0.3) (1.2.4)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0->turku-neural-parser==0.3) (0.16)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0->turku-neural-parser==0.3) (2.0.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0->turku-neural-parser==0.3) (2.9.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp==0.9.0->turku-neural-parser==0.3) (3.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py>=1.2.0->turku-neural-parser==0.3) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py>=1.2.0->turku-neural-parser==0.3) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py>=1.2.0->turku-neural-parser==0.3) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py>=1.2.0->turku-neural-parser==0.3) (4.6)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.6/dist-packages (from sphinxcontrib-websupport->sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0->turku-neural-parser==0.3) (1.1.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py>=1.2.0->turku-neural-parser==0.3) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py>=1.2.0->turku-neural-parser==0.3) (0.4.8)\n",
            "Building wheels for collected packages: configargparse, ufal.udpipe, word2number, parsimonious, overrides, ftfy, jsonnet, sacremoses\n",
            "  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configargparse: filename=ConfigArgParse-1.2.3-cp36-none-any.whl size=19329 sha256=682a401a6b5181a161448e301d74b447aaaf350bc48489aeeaf41f7c1b6fa8cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/d6/53/034032da9498bda2385cd50a51a289e88090b5da2d592b1fdf\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp36-cp36m-linux_x86_64.whl size=5625171 sha256=44fb0b2b3b3cc61fe5eeabb19f64616aa425855bb311d0b8153908873d113833\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5587 sha256=1e1f469313ee62d8829dc9d87247da437b0ef04f6b3dbd3cbe0bd2b414ab70ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42710 sha256=1b6c0193631e74fa00cf874937aa82b1d6e023818244341171240a98c642c8c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp36-none-any.whl size=10175 sha256=cd282eb9e83c1e1afd4a9df805f1555d336bf370eb64ad510217db7d8c77ab95\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45613 sha256=4c7fcef9455a458b422afef5e7f6c605edd51ff6a9abee2ea2224e4b989ce3f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp36-cp36m-linux_x86_64.whl size=3387897 sha256=359e6efba4cb319b4b982fa327fffe8508b830342953755adb82ad155162d2b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=c6302e7e852613b5fbca41f62cb4b783ae977de2bcf00e353c9dd3f26fac26fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built configargparse ufal.udpipe word2number parsimonious overrides ftfy jsonnet sacremoses\n",
            "\u001b[31mERROR: torchtext 0.8.1 has requirement torch==1.7.1, but you'll have torch 1.7.0+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.19.36 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: responses 0.12.1 has requirement urllib3>=1.25.10, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: opennmt-py 1.2.0 has requirement torchtext==0.4.0, but you'll have torchtext 0.8.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: configargparse, ufal.udpipe, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert, word2number, plac, preshed, blis, thinc, spacy, flaky, numpydoc, jsonpickle, parsimonious, overrides, flask-cors, ftfy, unidecode, jsonnet, sentencepiece, pytorch-transformers, conllu, responses, tensorboardX, zope.interface, greenlet, zope.event, gevent, allennlp, torchtext, waitress, pyonmttok, OpenNMT-py, sacremoses, tokenizers, transformers, turku-neural-parser\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: preshed 3.0.5\n",
            "    Uninstalling preshed-3.0.5:\n",
            "      Successfully uninstalled preshed-3.0.5\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed OpenNMT-py-1.2.0 allennlp-0.9.0 blis-0.2.4 boto3-1.16.36 botocore-1.19.36 configargparse-1.2.3 conllu-1.3.1 flaky-3.7.0 flask-cors-3.0.9 ftfy-5.8 gevent-20.9.0 greenlet-0.4.17 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-1.4.2 numpydoc-1.1.0 overrides-3.1.0 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pyonmttok-1.22.2 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.12.1 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.94 spacy-2.1.9 tensorboardX-2.1 thinc-7.0.8 tokenizers-0.7.0 torchtext-0.8.1 transformers-2.11.0 turku-neural-parser-0.3 ufal.udpipe-1.2.0.3 unidecode-1.1.1 waitress-1.4.4 word2number-1.1 zope.event-4.5.0 zope.interface-5.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue9vzD93xLyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be7fa887-48c6-4859-de4d-85dedbd9ca9d"
      },
      "source": [
        "!wget -nc http://dl.turkunlp.org/turku-parser-models/models_fi_tdt_v2.7.tar.gz\n",
        "!tar zxvf models_fi_tdt_v2.7.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-15 10:18:39--  http://dl.turkunlp.org/turku-parser-models/models_fi_tdt_v2.7.tar.gz\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 590212039 (563M) [application/octet-stream]\n",
            "Saving to: ‘models_fi_tdt_v2.7.tar.gz’\n",
            "\n",
            "models_fi_tdt_v2.7. 100%[===================>] 562.87M  4.84MB/s    in 80s     \n",
            "\n",
            "2020-12-15 10:19:59 (7.07 MB/s) - ‘models_fi_tdt_v2.7.tar.gz’ saved [590212039/590212039]\n",
            "\n",
            "models_fi_tdt_v2.7/\n",
            "models_fi_tdt_v2.7/pipelines.yaml\n",
            "models_fi_tdt_v2.7/Tokenizer/\n",
            "models_fi_tdt_v2.7/Tokenizer/tokenizer.udpipe\n",
            "models_fi_tdt_v2.7/Lemmatizer/\n",
            "models_fi_tdt_v2.7/Lemmatizer/big_lemma_cache.tsv\n",
            "models_fi_tdt_v2.7/Lemmatizer/lemma_cache.tsv\n",
            "models_fi_tdt_v2.7/Lemmatizer/lemmatizer.pt\n",
            "models_fi_tdt_v2.7/Udify/\n",
            "models_fi_tdt_v2.7/Udify/model.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpRs20VfbLT2"
      },
      "source": [
        "# Running the parser\n",
        "\n",
        "* Every model can specify many processing pipelines\n",
        "* These are in `modeldir/pipelines.yaml`\n",
        "* `parse_plaintext`is the default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jCpvDULxW8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acfcd376-894c-479d-938a-7d0bcca3a8b6"
      },
      "source": [
        "from tnparser.pipeline import read_pipelines, Pipeline\n",
        "\n",
        "available_pipelines=read_pipelines(\"models_fi_tdt_v2.7/pipelines.yaml\")\n",
        "print(list(available_pipelines.keys()))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['parse_plaintext', 'parse_sentlines', 'parse_wslines', 'parse_conllu', 'tokenize', 'parse_noisytext']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeFgUzS5dLKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adca3b1b-3673-478d-cd2c-6d3499cb3b9d"
      },
      "source": [
        "#this one will take long on first run because of loading the model\n",
        "p=Pipeline(available_pipelines[\"parse_plaintext\"])\n",
        "parsed=p.parse(\"Minulla on ruskea koira! Se haukkuu ja juoksee. Voi että!\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n",
            "Dataset reader: <class 'tnparser.udify.dataset_readers.universal_dependencies.UniversalDependenciesDatasetReader'>\n",
            "0it [00:00, ?it/s]Your label namespace was 'upos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "Your label namespace was 'xpos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "Your label namespace was 'feats'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "Your label namespace was 'lemmas'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "3it [00:00, 117.12it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/allennlp/data/token_indexers/token_indexer.py:113: FutureWarning: Using a Field with pad_token_sequence, which will be depreciated in 1.0.0.Please implement as_padded_tensor instead.\n",
            "  \"Please implement as_padded_tensor instead.\", FutureWarning)\n",
            "Encountered the logits key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "Encountered the class_probabilities key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "Encountered the arc_loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "Encountered the tag_loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "Encountered the loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            " >>> 0/13 lemmas already filled before lemma cache module\n",
            " >>> 13/13 lemmatized with lemma cache\n",
            " >>> 0/13 unique tokens submitted to lemmatizer\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqZokOvhd7KW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e40e464f-09ce-4a54-9e5b-57c1858e0b8f"
      },
      "source": [
        "print(parsed)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# newdoc\n",
            "# newpar\n",
            "# sent_id = 1\n",
            "# text = Minulla on ruskea koira!\n",
            "1\tMinulla\tminä\tPRON\t_\tCase=Ade|Number=Sing|Person=1|PronType=Prs\t0\troot\t_\t_\n",
            "2\ton\tolla\tAUX\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t1\tcop:own\t_\t_\n",
            "3\truskea\truskea\tADJ\t_\tCase=Nom|Degree=Pos|Number=Sing\t4\tamod\t_\t_\n",
            "4\tkoira\tkoira\tNOUN\t_\tCase=Nom|Number=Sing\t1\tnsubj:cop\t_\t_\n",
            "5\t!\t!\tPUNCT\t_\t_\t1\tpunct\t_\t_\n",
            "\n",
            "# sent_id = 2\n",
            "# text = Se haukkuu ja juoksee.\n",
            "1\tSe\tse\tPRON\t_\tCase=Nom|Number=Sing|PronType=Dem\t2\tnsubj\t_\t_\n",
            "2\thaukkuu\thaukkua\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\n",
            "3\tja\tja\tCCONJ\t_\t_\t4\tcc\t_\t_\n",
            "4\tjuoksee\tjuosta\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t2\tconj\t_\t_\n",
            "5\t.\t.\tPUNCT\t_\t_\t2\tpunct\t_\t_\n",
            "\n",
            "# sent_id = 3\n",
            "# text = Voi että!\n",
            "1\tVoi\tvoi\tINTJ\t_\t_\t2\tdiscourse\t_\t_\n",
            "2\tettä\tettä\tINTJ\t_\t_\t0\troot\t_\t_\n",
            "3\t!\t!\tPUNCT\t_\t_\t2\tpunct\t_\t_\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_QX7QvL7YUF"
      },
      "source": [
        "## Addition to the original notebook (H.K.)\r\n",
        "\r\n",
        "How to parse txt-files.\r\n",
        "\r\n",
        "Add the file to Colab session folder and open it. [How to open and read a file](https://colab.research.google.com/github/computationalcore/introduction-to-python/blob/master/notebooks/4-files/PY0101EN-4-1-ReadFile.ipynb#scrollTo=JeQkSJ5Kx2_U).\r\n",
        "\r\n",
        "Note! All the added and written files will disappear when the Colab session ends. Remember to download parsed files to save them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f2xmsbApK8R",
        "outputId": "41d0caa9-0d28-4848-fb4d-9f031d3bb4bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# import a txt file for analysis\r\n",
        "\r\n",
        "filepath = \"/content/testi.txt\" # path to a file in session folder\r\n",
        "my_file = open(filepath)\r\n",
        "my_file.name\r\n",
        "\r\n",
        "file_content = my_file.read()\r\n",
        "print(file_content)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3. Kirjautuminen Moodleen\n",
            "A. Turun yliopiston tunnuksella\n",
            "\n",
            "Turun yliopiston opiskelijat (myös avoimen yliopiston opiskelijat) ja henkilökunta kirjautuvat Moodleen Turun yliopiston tunnuksella eli UTU-tunnuksella ja salasanalla.\n",
            "\n",
            "Avoimen yliopiston opiskelijat saavat UTU-käyttäjätunnuksen  ja salasanan käyttöön ilmoittautumisen yhteydessä avoimen opintoihin Avoimen yliopiston Nettiopsussa (https://nettiopsu.utu.fi/avoin). Huom! Kirjautuminen Moodleen onnistuu vasta kurssihyväksynnästä seuraavana päivänä, kun opinto-oikeus on käyttäjätunnusten hallinnan tiedossa, ellei muuta perustetta käyttäjätunnukseen ole. Jos maksat opintomaksun lauantaina, sunnuntaina tai maanantaina, saat Moodlen käyttöösi tiistaina.\n",
            "\n",
            "1. Moodlen etusivulla https://moodle.utu.fi valitse kirjautumisvaihtoehdoksi kirjaudu Turun yliopiston tunnuksella. Voit myös kirjautua oikeasta yläkulmasta painamalla \"kirjaudu\" ja valitsemalla avautuvasta ikkunasta Turun yliopiston tunnuksella.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiEUylTW7WeM"
      },
      "source": [
        "parsed_file=p.parse(file_content)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS5IoF5975Cr",
        "outputId": "6d86d203-40ec-4bcf-d007-0fa599595358",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(parsed_file)\r\n",
        "my_file.close()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# newdoc\n",
            "# newpar\n",
            "# sent_id = 1\n",
            "# text = 3. Kirjautuminen Moodleen A. Turun yliopiston tunnuksella\n",
            "1\t3.\t3.\tADJ\t_\tNumType=Ord\t2\tnmod\t_\t_\n",
            "2\tKirjautuminen\tkirjautuminen\tNOUN\t_\tCase=Nom|Derivation=Minen|Number=Sing\t0\troot\t_\t_\n",
            "3\tMoodleen\tMoodle\tPROPN\t_\tCase=Ill|Number=Sing\t2\tnmod\t_\t_\n",
            "4\tA.\tA.\tNOUN\t_\tAbbr=Yes|Case=Nom|Number=Sing\t2\tnmod\t_\t_\n",
            "5\tTurun\tTurku\tPROPN\t_\tCase=Gen|Number=Sing\t6\tnmod:poss\t_\t_\n",
            "6\tyliopiston\tyli#opisto\tNOUN\t_\tCase=Gen|Number=Sing\t7\tnmod:poss\t_\t_\n",
            "7\ttunnuksella\ttunnus\tNOUN\t_\tCase=Ade|Number=Sing\t2\tnmod\t_\t_\n",
            "\n",
            "# newpar\n",
            "# sent_id = 2\n",
            "# text = Turun yliopiston opiskelijat (myös avoimen yliopiston opiskelijat) ja henkilökunta kirjautuvat Moodleen Turun yliopiston tunnuksella eli UTU-tunnuksella ja salasanalla.\n",
            "1\tTurun\tTurku\tPROPN\t_\tCase=Gen|Number=Sing\t2\tnmod:poss\t_\t_\n",
            "2\tyliopiston\tyli#opisto\tNOUN\t_\tCase=Gen|Number=Sing\t3\tnmod:poss\t_\t_\n",
            "3\topiskelijat\topiskelija\tNOUN\t_\tCase=Nom|Derivation=Ja|Number=Plur\t12\tnsubj\t_\t_\n",
            "4\t(\t(\tPUNCT\t_\t_\t8\tpunct\t_\t_\n",
            "5\tmyös\tmyös\tADV\t_\t_\t8\tadvmod\t_\t_\n",
            "6\tavoimen\tavoin\tADJ\t_\tCase=Gen|Degree=Pos|Number=Sing\t7\tamod\t_\t_\n",
            "7\tyliopiston\tyli#opisto\tNOUN\t_\tCase=Gen|Number=Sing\t8\tnmod:poss\t_\t_\n",
            "8\topiskelijat\topiskelija\tNOUN\t_\tCase=Nom|Derivation=Ja|Number=Plur\t3\tappos\t_\t_\n",
            "9\t)\t)\tPUNCT\t_\t_\t8\tpunct\t_\t_\n",
            "10\tja\tja\tCCONJ\t_\t_\t11\tcc\t_\t_\n",
            "11\thenkilökunta\thenkilö#kunta\tNOUN\t_\tCase=Nom|Number=Sing\t3\tconj\t_\t_\n",
            "12\tkirjautuvat\tkirjautua\tVERB\t_\tMood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\n",
            "13\tMoodleen\tMoodle\tPROPN\t_\tCase=Ill|Number=Sing\t12\tobl\t_\t_\n",
            "14\tTurun\tTurku\tPROPN\t_\tCase=Gen|Number=Sing\t15\tnmod:poss\t_\t_\n",
            "15\tyliopiston\tyli#opisto\tNOUN\t_\tCase=Gen|Number=Sing\t16\tnmod:poss\t_\t_\n",
            "16\ttunnuksella\ttunnus\tNOUN\t_\tCase=Ade|Number=Sing\t12\tobl\t_\t_\n",
            "17\teli\teli\tCCONJ\t_\t_\t18\tcc\t_\t_\n",
            "18\tUTU-tunnuksella\tUTU#tunnus\tNOUN\t_\tCase=Ade|Number=Sing\t16\tconj\t_\t_\n",
            "19\tja\tja\tCCONJ\t_\t_\t20\tcc\t_\t_\n",
            "20\tsalasanalla\tsala#sana\tNOUN\t_\tCase=Ade|Number=Sing\t16\tconj\t_\t_\n",
            "21\t.\t.\tPUNCT\t_\t_\t12\tpunct\t_\t_\n",
            "\n",
            "# newpar\n",
            "# sent_id = 3\n",
            "# text = Avoimen yliopiston opiskelijat saavat UTU-käyttäjätunnuksen ja salasanan käyttöön ilmoittautumisen yhteydessä avoimen opintoihin\n",
            "1\tAvoimen\tavoin\tADJ\t_\tCase=Gen|Degree=Pos|Number=Sing\t2\tamod\t_\t_\n",
            "2\tyliopiston\tyli#opisto\tNOUN\t_\tCase=Gen|Number=Sing\t3\tnmod:poss\t_\t_\n",
            "3\topiskelijat\topiskelija\tNOUN\t_\tCase=Nom|Derivation=Ja|Number=Plur\t4\tnsubj\t_\t_\n",
            "4\tsaavat\tsaada\tVERB\t_\tMood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\n",
            "5\tUTU-käyttäjätunnuksen\tUTU#käyttä#jätunnus\tNOUN\t_\tCase=Gen|Number=Sing\t4\tobj\t_\t_\n",
            "6\tja\tja\tCCONJ\t_\t_\t7\tcc\t_\t_\n",
            "7\tsalasanan\tsala#sana\tNOUN\t_\tCase=Gen|Number=Sing\t5\tconj\t_\t_\n",
            "8\tkäyttöön\tkäyttö\tNOUN\t_\tCase=Ill|Number=Sing\t9\tnmod\t_\t_\n",
            "9\tilmoittautumisen\tilmoittautuminen\tNOUN\t_\tCase=Gen|Derivation=Minen|Number=Sing\t10\tnmod:poss\t_\t_\n",
            "10\tyhteydessä\tyhteys\tNOUN\t_\tCase=Ine|Derivation=Vs|Number=Sing\t4\tobl\t_\t_\n",
            "11\tavoimen\tavoin\tADJ\t_\tCase=Gen|Number=Sing\t12\tnmod:poss\t_\t_\n",
            "12\topintoihin\topinto\tNOUN\t_\tCase=Ill|Number=Plur\t10\tnmod\t_\t_\n",
            "\n",
            "# sent_id = 4\n",
            "# text = Avoimen yliopiston Nettiopsussa (https://nettiopsu.utu.fi/avoin).\n",
            "1\tAvoimen\tavoin\tADJ\t_\tCase=Gen|Degree=Pos|Number=Sing\t2\tamod\t_\t_\n",
            "2\tyliopiston\tyli#opisto\tNOUN\t_\tCase=Gen|Number=Sing\t3\tnmod:poss\t_\t_\n",
            "3\tNettiopsussa\tnetti#opsu\tNOUN\t_\tCase=Ine|Number=Sing\t0\troot\t_\t_\n",
            "4\t(\t(\tPUNCT\t_\t_\t5\tpunct\t_\t_\n",
            "5\thttps://nettiopsu.utu.fi/avoin\thttps://nettiopsu.utu.fi/avoin\tSYM\t_\t_\t3\tappos\t_\t_\n",
            "6\t)\t)\tPUNCT\t_\t_\t5\tpunct\t_\t_\n",
            "7\t.\t.\tPUNCT\t_\t_\t3\tpunct\t_\t_\n",
            "\n",
            "# sent_id = 5\n",
            "# text = Huom!\n",
            "1\tHuom\tHuom\tNOUN\t_\tAbbr=Yes\t0\troot\t_\t_\n",
            "2\t!\t!\tPUNCT\t_\t_\t1\tpunct\t_\t_\n",
            "\n",
            "# sent_id = 6\n",
            "# text = Kirjautuminen Moodleen onnistuu vasta kurssihyväksynnästä seuraavana päivänä, kun opinto-oikeus on käyttäjätunnusten hallinnan tiedossa, ellei muuta perustetta käyttäjätunnukseen ole.\n",
            "1\tKirjautuminen\tkirjautuminen\tNOUN\t_\tCase=Nom|Derivation=Minen|Number=Sing\t3\tnsubj\t_\t_\n",
            "2\tMoodleen\tMoodle\tPROPN\t_\tCase=Ill|Number=Sing\t1\tnmod\t_\t_\n",
            "3\tonnistuu\tonnistua\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\n",
            "4\tvasta\tvasta\tADV\t_\t_\t7\tadvmod\t_\t_\n",
            "5\tkurssihyväksynnästä\tkurssi#hyväksyntä\tNOUN\t_\tCase=Ela|Number=Sing\t6\tobl\t_\t_\n",
            "6\tseuraavana\tseurata\tVERB\t_\tCase=Ess|Degree=Pos|Number=Sing\t7\tacl\t_\t_\n",
            "7\tpäivänä\tpäivä\tNOUN\t_\tCase=Ess|Number=Sing\t3\tobl\t_\t_\n",
            "8\t,\t,\tPUNCT\t_\t_\t14\tpunct\t_\t_\n",
            "9\tkun\tkun\tSCONJ\t_\t_\t14\tmark\t_\t_\n",
            "10\topinto-oikeus\topinto#oikeus\tNOUN\t_\tCase=Nom|Derivation=Vs|Number=Sing\t14\tnsubj:cop\t_\t_\n",
            "11\ton\tolla\tAUX\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t14\tcop\t_\t_\n",
            "12\tkäyttäjätunnusten\tkäyttäjä#tunnus\tNOUN\t_\tCase=Gen|Number=Plur\t13\tnmod:gobj\t_\t_\n",
            "13\thallinnan\thallinta\tNOUN\t_\tCase=Gen|Number=Sing\t14\tnmod:poss\t_\t_\n",
            "14\ttiedossa\ttieto\tNOUN\t_\tCase=Ine|Number=Sing\t3\tadvcl\t_\t_\n",
            "15\t,\t,\tPUNCT\t_\t_\t21\tpunct\t_\t_\n",
            "16-17\tellei\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "16\tjos\tjos\tSCONJ\t_\t_\t21\tmark\t_\t_\n",
            "17\tei\tei\tAUX\t_\tNumber=Sing|Person=3|Polarity=Neg|VerbForm=Fin|Voice=Act\t21\taux\t_\t_\n",
            "18\tmuuta\tmuu\tPRON\t_\tCase=Par|Number=Sing|PronType=Ind\t19\tdet\t_\t_\n",
            "19\tperustetta\tperuste\tNOUN\t_\tCase=Par|Number=Sing\t21\tnsubj\t_\t_\n",
            "20\tkäyttäjätunnukseen\tkäyttäjä#tunnus\tNOUN\t_\tCase=Ill|Number=Sing\t19\tnmod\t_\t_\n",
            "21\tole\tolla\tVERB\t_\tConnegative=Yes|Mood=Ind|Tense=Pres|VerbForm=Fin\t14\tadvcl\t_\t_\n",
            "22\t.\t.\tPUNCT\t_\t_\t3\tpunct\t_\t_\n",
            "\n",
            "# sent_id = 7\n",
            "# text = Jos maksat opintomaksun lauantaina, sunnuntaina tai maanantaina, saat Moodlen käyttöösi tiistaina.\n",
            "1\tJos\tjos\tSCONJ\t_\t_\t2\tmark\t_\t_\n",
            "2\tmaksat\tmaksaa\tVERB\t_\tMood=Ind|Number=Sing|Person=2|Tense=Pres|VerbForm=Fin|Voice=Act\t10\tadvcl\t_\t_\n",
            "3\topintomaksun\topinto#maksu\tNOUN\t_\tCase=Gen|Derivation=U|Number=Sing\t2\tobj\t_\t_\n",
            "4\tlauantaina\tlauantai\tNOUN\t_\tCase=Ess|Number=Sing\t2\tobl\t_\t_\n",
            "5\t,\t,\tPUNCT\t_\t_\t6\tpunct\t_\t_\n",
            "6\tsunnuntaina\tsunnuntai\tNOUN\t_\tCase=Ess|Number=Sing\t4\tconj\t_\t_\n",
            "7\ttai\ttai\tCCONJ\t_\t_\t8\tcc\t_\t_\n",
            "8\tmaanantaina\tmaanantai\tNOUN\t_\tCase=Ess|Number=Sing\t4\tconj\t_\t_\n",
            "9\t,\t,\tPUNCT\t_\t_\t2\tpunct\t_\t_\n",
            "10\tsaat\tsaada\tVERB\t_\tMood=Ind|Number=Sing|Person=2|Tense=Pres|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\n",
            "11\tMoodlen\tMoodle\tPROPN\t_\tCase=Gen|Number=Sing\t10\tobj\t_\t_\n",
            "12\tkäyttöösi\tkäyttö\tNOUN\t_\tCase=Par|Number=Sing|Number[psor]=Sing|Person[psor]=2\t10\tobl\t_\t_\n",
            "13\ttiistaina\ttiistai\tNOUN\t_\tCase=Ess|Number=Sing\t10\tobl\t_\t_\n",
            "14\t.\t.\tPUNCT\t_\t_\t10\tpunct\t_\t_\n",
            "\n",
            "# newpar\n",
            "# sent_id = 8\n",
            "# text = 1. Moodlen etusivulla https://moodle.utu.fi valitse kirjautumisvaihtoehdoksi kirjaudu Turun yliopiston tunnuksella.\n",
            "1\t1.\t1.\tADJ\t_\tNumType=Ord\t5\tobl\t_\t_\n",
            "2\tMoodlen\tMoodle\tPROPN\t_\tCase=Gen|Number=Sing\t3\tnmod:poss\t_\t_\n",
            "3\tetusivulla\tetu#sivu\tNOUN\t_\tCase=Ade|Number=Sing\t5\tobl\t_\t_\n",
            "4\thttps://moodle.utu.fi\thttps://moodle.utu.fi\tSYM\t_\t_\t3\tappos\t_\t_\n",
            "5\tvalitse\tvalita\tVERB\t_\tMood=Imp|Number=Sing|Person=2|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\n",
            "6\tkirjautumisvaihtoehdoksi\tkirjautumis#vaihto#ehto\tNOUN\t_\tCase=Tra|Number=Sing\t5\txcomp:ds\t_\t_\n",
            "7\tkirjaudu\tkirjaudu\tVERB\t_\tCase=Nom|Derivation=Minen|Number=Sing\t5\tobj\t_\t_\n",
            "8\tTurun\tTurku\tPROPN\t_\tCase=Gen|Number=Sing\t9\tnmod:poss\t_\t_\n",
            "9\tyliopiston\tyli#opisto\tNOUN\t_\tCase=Gen|Number=Sing\t10\tnmod:poss\t_\t_\n",
            "10\ttunnuksella\ttunnus\tNOUN\t_\tCase=Ade|Number=Sing\t7\tobl\t_\t_\n",
            "11\t.\t.\tPUNCT\t_\t_\t5\tpunct\t_\t_\n",
            "\n",
            "# sent_id = 9\n",
            "# text = Voit myös kirjautua oikeasta yläkulmasta painamalla \"kirjaudu\" ja valitsemalla avautuvasta ikkunasta Turun yliopiston tunnuksella.\n",
            "1\tVoit\tvoida\tAUX\t_\tMood=Ind|Number=Sing|Person=2|Tense=Pres|VerbForm=Fin|Voice=Act\t3\taux\t_\t_\n",
            "2\tmyös\tmyös\tADV\t_\t_\t3\tadvmod\t_\t_\n",
            "3\tkirjautua\tkirjautua\tVERB\t_\tInfForm=1|Number=Sing|VerbForm=Inf|Voice=Act\t0\troot\t_\t_\n",
            "4\toikeasta\toikea\tADJ\t_\tCase=Ela|Degree=Pos|Number=Sing\t5\tamod\t_\t_\n",
            "5\tyläkulmasta\tylä#kulma\tNOUN\t_\tCase=Ela|Number=Sing\t3\tobl\t_\t_\n",
            "6\tpainamalla\tpainaa\tVERB\t_\tCase=Ade|InfForm=3|Number=Sing|VerbForm=Inf|Voice=Act\t3\tadvcl\t_\t_\n",
            "7\t\"\t\"\tPUNCT\t_\t_\t8\tpunct\t_\t_\n",
            "8\tkirjaudu\tkirjautua\tVERB\t_\tMood=Imp|Number=Sing|Person=2|VerbForm=Fin|Voice=Act\t6\tparataxis\t_\t_\n",
            "9\t\"\t\"\tPUNCT\t_\t_\t8\tpunct\t_\t_\n",
            "10\tja\tja\tCCONJ\t_\t_\t11\tcc\t_\t_\n",
            "11\tvalitsemalla\tvalita\tVERB\t_\tCase=Ade|InfForm=3|Number=Sing|VerbForm=Inf|Voice=Act\t6\tconj\t_\t_\n",
            "12\tavautuvasta\tavautua\tVERB\t_\tCase=Ela|Degree=Pos|Number=Sing|PartForm=Pres|VerbForm=Part|Voice=Act\t13\tacl\t_\t_\n",
            "13\tikkunasta\tikkuna\tNOUN\t_\tCase=Ela|Number=Sing\t11\tobl\t_\t_\n",
            "14\tTurun\tTurku\tPROPN\t_\tCase=Gen|Number=Sing\t15\tnmod:poss\t_\t_\n",
            "15\tyliopiston\tyli#opisto\tNOUN\t_\tCase=Gen|Number=Sing\t16\tnmod:poss\t_\t_\n",
            "16\ttunnuksella\ttunnus\tNOUN\t_\tCase=Ade|Number=Sing\t11\tobl\t_\t_\n",
            "17\t.\t.\tPUNCT\t_\t_\t3\tpunct\t_\t_\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VW3UDHq-PFB"
      },
      "source": [
        "# Save parsed conllu file to session folder. Remember to download it if you wish to save it!\r\n",
        "\r\n",
        "with open('/content/testiconllu.txt', 'w') as writefile:\r\n",
        "    writefile.write(parsed_file)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bXEDdThfM-Z"
      },
      "source": [
        "# GPU mode\n",
        "\n",
        "* The pipeline runs by default in CPU mode\n",
        "* Needs to be told to run in GPU\n",
        "* This is a bit tricky right now but not impossible\n",
        "* Note: if you now switch the Runtime into GPU, you need to re-run the pip install\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt3slQjCfoEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac831067-ba02-45bf-b491-2e98ab92c9b0"
      },
      "source": [
        "#I do realize this ain't good! :)\n",
        "import types\n",
        "extra_args=types.SimpleNamespace()\n",
        "extra_args.__dict__[\"udify_mod.device\"]=\"0\" #simulates someone giving a --device 0 parameter to Udify\n",
        "extra_args.__dict__[\"lemmatizer_mod.device\"]=\"0\" \n",
        "\n",
        "p=Pipeline(available_pipelines[\"parse_plaintext\"],extra_args)\n",
        "parsed=p.parse(\"Minulla on ruskea koira! Se haukkuu ja juoksee. Voi että!\")\n",
        "print(\"Parsed has this many lines:\",len(parsed.split(\"\\n\")))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n",
            "Dataset reader: <class 'tnparser.udify.dataset_readers.universal_dependencies.UniversalDependenciesDatasetReader'>\n",
            "0it [00:00, ?it/s]Your label namespace was 'upos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "Your label namespace was 'xpos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "Your label namespace was 'feats'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "Your label namespace was 'lemmas'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "3it [00:00, 240.06it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/allennlp/data/token_indexers/token_indexer.py:113: FutureWarning: Using a Field with pad_token_sequence, which will be depreciated in 1.0.0.Please implement as_padded_tensor instead.\n",
            "  \"Please implement as_padded_tensor instead.\", FutureWarning)\n",
            "Encountered the logits key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "Encountered the class_probabilities key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "Encountered the arc_loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "Encountered the tag_loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "Encountered the loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            " >>> 0/13 lemmas already filled before lemma cache module\n",
            " >>> 13/13 lemmatized with lemma cache\n",
            " >>> 0/13 unique tokens submitted to lemmatizer\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Parsed has this many lines: 25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "600it [00:00, 5055.19it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRGxN0bN1Nev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d9aaa3-8b53-4ba2-ac17-246a754adb5b"
      },
      "source": [
        "#Since we are on a GPU, we can try to push through quite a bit more of data\n",
        "parsed=p.parse(\"Minulla on ruskea koira! Se haukkuu ja juoksee. Voi että! \"*200) #takes forever on CPU, finishes in few seconds on GPU\n",
        "print(\"Parsed has this many lines:\",len(parsed.split(\"\\n\")))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsed has this many lines: 4403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G90KC9dFfp-h"
      },
      "source": [
        "# Process the output\n",
        "\n",
        "* The output of the pipeline run is a conll-u string\n",
        "* You can parse it in any number of ways\n",
        "* This is my preferred:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enQ-xvnff599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e84ce81c-63ae-47b4-8bad-8a57fb2c6559"
      },
      "source": [
        "ID,FORM,LEMMA,UPOS,XPOS,FEAT,HEAD,DEPREL,DEPS,MISC=range(10) #the 10 columns\n",
        "\n",
        "def read_conll(inp,max_sent=0,drop_tokens=True,drop_nulls=True):\n",
        "    \"\"\"\n",
        "    inp: list of lines or an open file\n",
        "    max_sent: 0 for all, >0 to limit\n",
        "    drop_tokens: ignore multiword token lines\n",
        "    drop_nulls: ignore null nodes in enhanced dependencies\n",
        "\n",
        "    Yields lines of the parse and comments\n",
        "    \"\"\"\n",
        "\n",
        "    comments=[]\n",
        "    sent=[]\n",
        "    yielded=0\n",
        "    for line in inp:\n",
        "        line=line.rstrip(\"\\n\")\n",
        "        if line.startswith(\"#\"):\n",
        "            comments.append(line)\n",
        "        elif not line:\n",
        "            if sent:\n",
        "                yield sent,comments\n",
        "                yielded+=1\n",
        "                if max_sent>0 and yielded==max_sent:\n",
        "                    break\n",
        "                sent,comments=[],[]\n",
        "        else:\n",
        "            cols=line.split(\"\\t\")\n",
        "            if drop_tokens and \"-\" in cols[ID]:\n",
        "                continue\n",
        "            if drop_nulls and \".\" in cols[ID]:\n",
        "                continue\n",
        "            sent.append(cols)\n",
        "    else:\n",
        "        if sent:\n",
        "            yield sent,comments\n",
        "\n",
        "for one_sent,comments in read_conll(parsed.split(\"\\n\"),5):\n",
        "    words=(word_line[FORM] for word_line in one_sent)\n",
        "    lemmas=(word_line[LEMMA] for word_line in one_sent)\n",
        "    print(\" \".join(words))\n",
        "    print(\" \".join(lemmas))\n",
        "    print()\n",
        "\n",
        "# and that's really all there is to it :)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minulla on ruskea koira !\n",
            "minä olla ruskea koira !\n",
            "\n",
            "Se haukkuu ja juoksee .\n",
            "se haukkua ja juosta .\n",
            "\n",
            "Voi että !\n",
            "voi että !\n",
            "\n",
            "Minulla on ruskea koira !\n",
            "minä olla ruskea koira !\n",
            "\n",
            "Se haukkuu ja juoksee .\n",
            "se haukkua ja juosta .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}