{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "bert_text_classification.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqKV5-C0AUJ5"
      },
      "source": [
        "# Text classification with BERT\n",
        "\n",
        "This notebook briefly demonstrates fine-tuning a pretrained BERT model to a text classification task.\n",
        "\n",
        "You probably want to run this notebook with GPU acceleration, as fine-tuning BERT on CPU can be fairly slow even with a comparatively small dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w6S22LKAUJ6"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "The following variables configure a few aspects of the data, model, and training process. To adapt this example to a different dataset, you'll probably want to change these to match.\n",
        "\n",
        "Note in particular that we're limiting the number of examples and the maximum sequence length to make training faster. \n",
        "\n",
        "When running on GPU, it's also necessary to make sure that the input length and batch size are not so large as to cause a batch to exceed GPU memory. If you're getting a message like `Resource exhausted: OOM when allocating tensor`, try smaller numbers for these."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyp2_wPqAUKQ",
        "outputId": "5f81784c-1102-4593-fc83-075df3741fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import os\n",
        "os.environ['TF_KERAS'] = '1'    # Required to use tensorflow.python.keras with keras-bert'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsibdKlqAUJ-"
      },
      "source": [
        "# Maximum number of examples to read\n",
        "MAX_EXAMPLES = 5000\n",
        "\n",
        "# Maximum length of input sequence in tokens\n",
        "INPUT_LENGTH = 250\n",
        "\n",
        "# Number of epochs to train for\n",
        "EPOCHS = 3\n",
        "\n",
        "# Optimizer learning rate\n",
        "LEARNING_RATE = 0.00002\n",
        "\n",
        "# Training batch size\n",
        "BATCH_SIZE = 8"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRKMuG63AUKG"
      },
      "source": [
        "## Package setup\n",
        "\n",
        "We'll use [keras-bert](https://github.com/CyberZHG/keras-bert). Make sure the package is installed. (`pip` is the Python [package installer](https://pip.pypa.io/en/stable/))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rqfN5PBAUKH"
      },
      "source": [
        "!pip3 install keras-bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAvXsmOzAUKO"
      },
      "source": [
        "## Tensorflow configuration\n",
        "\n",
        "We'll need set an environment variable for keras-bert to use `tensorflow.python.keras`. (This is a technical detail that is not related to the model.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bpk1fs2DAUKV"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "We'll use the IMDB dataset in JSON format. (This should be familiar to you from previous notebooks in this course.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUzAjvJ_AUKW",
        "outputId": "41517fcf-e004-4a94-8bdd-77845520e099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "#Gdown is a utility for downloading large files from Google Drive, where I mirrored the NER trained model for you\n",
        "\n",
        "!pip install gdown\n",
        "!gdown -O fincore_5k_sample.txt 'https://drive.google.com/uc?export=download&id=1-1J3ieUyYsRj2TGvsmimxh7RAQFndaVu'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1-1J3ieUyYsRj2TGvsmimxh7RAQFndaVu\n",
            "To: /content/fincore_5k_sample.txt\n",
            "4.65MB [00:00, 72.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lHzfhBNAUKa"
      },
      "source": [
        "## Load data\n",
        "\n",
        "Load a sample of the FinCORE corpus. It comes in the form \"label TAB text\". There is 8 main labels and another about 30 combinations of these 8 main labels (multilabel problem)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l7L9MGMAUKk"
      },
      "source": [
        "Separate the texts and labels into lists of their own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckZw41L0AUKk",
        "outputId": "e14dda19-be0b-4e25-d065-590ae4830e46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import random\n",
        "random.seed(1234)\n",
        "texts=[]\n",
        "labels=[]\n",
        "with open(\"fincore_5k_sample.txt\") as f:\n",
        "    lines=f.readlines()\n",
        "    random.shuffle(lines)\n",
        "    for line in lines[:MAX_EXAMPLES]:\n",
        "        line=line.rstrip(\"\\n\")\n",
        "        lab,txt=line.split(\"\\t\")\n",
        "        texts.append(txt)\n",
        "        labels.append(lab)\n",
        "\n",
        "# Example text and label\n",
        "print('Text:', texts[0])\n",
        "print('Label:', labels[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: Harjoittelu Alla on Psylin opiskelijatoimikunnan määritelmä harjoittelun tavoitteista sekä Psylin palkkasuositus , joka tulee pitää mielessä harjoittelupaikkaa etsiessä . Harjoittelun ei tule olla huonosti palkattua saati palkatonta . Palkkojen poljenta ei aja minkään tahon etuja ja vastaavasti palkka-asioiden hoitaminen suositusten mukaisesti on signaali kestävästä ja ammattitaitoa arvostavasta kehityksestä . HARJOITTELUN TAVOITTEET JA SISÄLLÖT Harjoittelun keskeisenä tavoitteena on kehittää psykologin työtehtävissä tarvittavia ammatillisia valmiuksia sekä ammatti-identiteettiä . Harjoittelussa opiskelija perehtyy yksilön ja yhteisön arviointiin , ohjaus- , neuvonta- , hoito- ja kuntoutusmenetelmiin , psykologisten palvelujen järjestelmien toimintaan sekä psykologin työn eettisiin lähtökohtiin ja velvoitteisiin . Hän oppii käyttämään jo omaksumiaan tietoja ja taitoja käytännön työtilanteessa , oppii uusia työtaitoja ja tietää , mitä tietoja ja taitoja hän tarvitsee voidakseen kehittyä työntekijänä . Harjoittelija oppii työskentelemään yhteistyössä muiden ammattiryhmien kanssa\n",
            "Label: IN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJfHGxZDAUKp"
      },
      "source": [
        "## Download pretrained BERT model\n",
        "\n",
        "As training BERT from scratch generally takes days, we'll here load a pretrained model and fine-tune it for our task. URLs to download pre-trained models made available by Google are found at https://github.com/google-research/bert .\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odGmjZXJAUKq",
        "outputId": "1f43688d-3c3e-4b48-b553-a2d967d2e276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "# Give -nc (--no-clobber) argument so that the file isn't downloaded multiple times \n",
        "#!wget -nc http://dl.turkunlp.org/finbert/bert-base-finnish-cased-v1.zip\n",
        "\n",
        "!gdown -O bert-base-finnish-cased-v1.zip  'https://drive.google.com/uc?export=download&id=1y-5vAqyWP24Tm_VBsbaXBXnIddUKck-5'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1y-5vAqyWP24Tm_VBsbaXBXnIddUKck-5\n",
            "To: /content/bert-base-finnish-cased-v1.zip\n",
            "1.37GB [00:08, 155MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYjjov9AAUKw"
      },
      "source": [
        "Unpack the downloaded file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL-AdkHJAUKy",
        "outputId": "23421625-19e6-42bd-85d7-2d66e61fcad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "# Give -n argument so that existing files aren't overwritten \n",
        "!unzip -n bert-base-finnish-cased-v1.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  bert-base-finnish-cased-v1.zip\n",
            "  inflating: bert-base-finnish-cased-v1/bert_config.json  \n",
            "  inflating: bert-base-finnish-cased-v1/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: bert-base-finnish-cased-v1/bert_model.ckpt.index  \n",
            "  inflating: bert-base-finnish-cased-v1/bert_model.ckpt.meta  \n",
            "  inflating: bert-base-finnish-cased-v1/vocab.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ4K0mhpAUK2"
      },
      "source": [
        "There are three key parts to the package contents:\n",
        "\n",
        "* `vocab.txt`: plain text file listing vocabulary items\n",
        "* `bert_config.json`: model configuration in JSON format\n",
        "* `bert_model.ckpt.*`: model checkpoint data with pre-trained weights in [Tensorflow checkpoint format](https://www.tensorflow.org/guide/checkpoint)\n",
        "\n",
        "Take note of the path to these (you'll need to change this if you pick a different model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K135GkRJAUK3"
      },
      "source": [
        "bert_vocab_path = 'bert-base-finnish-cased-v1/vocab.txt'\n",
        "bert_config_path = 'bert-base-finnish-cased-v1/bert_config.json'\n",
        "bert_checkpoint_path = 'bert-base-finnish-cased-v1/bert_model.ckpt'    # suffixes not required"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuSU0qVBAUK7"
      },
      "source": [
        "Also take note if the model we downloaded was a case-sensitive (cased) or not. (This must match the model.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QVtE10BAUK7"
      },
      "source": [
        "model_is_cased = True"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gd413JxAULC"
      },
      "source": [
        "## Load BERT vocabulary\n",
        "\n",
        "This is just a plain text file with one vocabulary item per line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQhNDbonAULD",
        "outputId": "a86d2c4b-56e7-49bb-b9e3-0dc3157ad6b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "vocab = []\n",
        "with open(bert_vocab_path) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        vocab.append(line.rstrip('\\n'))    # rstrip to remove newline characters\n",
        "\n",
        "\n",
        "# Print a list with every 500th vocabulary item\n",
        "print(vocab[0::500])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[PAD]', '##hin', '##tila', 'tehtä', '##ce', 'avulla', '##uttua', 'Sil', '##iver', 'tarvits', 'kilometrin', 'sanotaan', '##ya', 'onnistuu', 'tapaus', 'rento', '##otin', '##kasvat', 'kauhe', 'puolin', 'ymmärrän', 'polttoain', 'arvot', 'ajattelu', '##impiin', 'huomasi', 'tietokoneen', 'tiedämme', 'johdossa', 'teemme', 'paikallisen', 'rekryt', 'vuosikymmeniä', 'kohdistuu', 'tyhmiä', 'baa', '##ipa', 'aero', '##ehtien', 'viimeisteli', '##llosta', 'luulevat', 'verenpain', 'tuottamaan', 'vahingot', 'opiskelijan', '##päivinä', '##uksellisesti', 'uskottava', '##elemaan', 'ilmennyt', 'määrätieto', 'leppo', 'yksityiset', 'kirjailijan', 'vastikään', 'samoista', '##misiin', '##pankkien', 'tuut', 'muutoinkin', 'säilyi', '##ivuoren', '##pauksia', 'kaupungilta', '##lalle', 'tervetulleeksi', 'lähteitä', 'isoin', 'Tuskinpa', 'hallinnut', '##kanslerin', '##jela', 'siniset', 'erikoiskokeella', 'todistavat', 'itsehallinto', 'Oran', 'vikaan', 'ISIS', 'hankinnan', '##eleitä', '##heitolla', '##upar', 'raam', 'edelläkävijä', 'tunnistettu', 'kohtaamis', 'punaisena', 'murtaa', '##ske', 'toimivuudesta', 'zo', '##ektro', '114', 'seinästä', 'Kuhmon', 'viesteissä', 'tukim', 'sellaisiksi', 'Rautio']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o07ntwK6AULH"
      },
      "source": [
        "## Load BERT configuration\n",
        "\n",
        "The configuration is just a JSON file, so we can read it in with `json.load` from the python `json` library.\n",
        "\n",
        "We won't actually need to use these configuration details directly (keras-bert takes care of them for us), so this is just here to show what information is contained in the config file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pisDxFcAULI",
        "outputId": "df1becfd-a861-4ed4-dc13-d7f772b5182d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "from pprint import pprint    # pretty-printer for output\n",
        "import json\n",
        "\n",
        "with open(bert_config_path) as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "\n",
        "# Print configuration contents\n",
        "pprint(config)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'attention_probs_dropout_prob': 0.1,\n",
            " 'hidden_act': 'gelu',\n",
            " 'hidden_dropout_prob': 0.1,\n",
            " 'hidden_size': 768,\n",
            " 'initializer_range': 0.02,\n",
            " 'intermediate_size': 3072,\n",
            " 'max_position_embeddings': 512,\n",
            " 'num_attention_heads': 12,\n",
            " 'num_hidden_layers': 12,\n",
            " 'type_vocab_size': 2,\n",
            " 'vocab_size': 50105}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho92sD6GAULO"
      },
      "source": [
        "## Create BERT tokenizer\n",
        "\n",
        "To create the tokenizer, we'll need a mapping from vocabulary items to their corresponding integer indices. We do this conventionally using `enumerate`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nd-FNg-AULP",
        "outputId": "1f1ea75a-fefd-49a0-c549-d666bb64315c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "# Create mapping from vocabulary items to their indices in the vocabulary\n",
        "token_dict = { v: i for i, v in enumerate(vocab) }\n",
        "\n",
        "\n",
        "# Print some random examples of the mapping\n",
        "pprint(dict(random.choices(list(token_dict.items()), k=10)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'##FO': 40995,\n",
            " '##itis': 24541,\n",
            " '##tiede': 36700,\n",
            " '##vä': 212,\n",
            " '##yhteydet': 23184,\n",
            " 'Rinteen': 29435,\n",
            " 'bus': 11652,\n",
            " 'parisuhdetta': 30697,\n",
            " 'taajuus': 46190,\n",
            " 'uusista': 11335}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFxTOnpsAULU"
      },
      "source": [
        "We'll use the keras-bert `Tokenizer` for BERT tokenization. The implementation supports\n",
        "\n",
        "* (Optional) lowercasing: `Hello` → `hello`\n",
        "* Basic tokenization: `Hello!` → `Hello` `!`, `multi-part` → `multi` `-` `part`\n",
        "* Wordpiece tokenization: `comprehensively` → `comprehensive` `##ly`\n",
        "* Adding special tokens: `Sentence`  → `[CLS]` `Sentence` `[SEP]`\n",
        "* Mapping to integer indices\n",
        "* Generating segment sequence\n",
        "* (Optional) padding and truncation to length\n",
        "\n",
        "In the following example, notice how words not in the dictionary are broken up into subwords (with continuation parts starting with `##`) and how unknown _characters_ are mapped to a special unknown word token `[UNK]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEzX3sR5AULV",
        "outputId": "398bc885-c262-4255-bf70-7d4f7a194cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "from keras_bert import Tokenizer\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(token_dict, cased=model_is_cased)\n",
        "\n",
        "\n",
        "# Let's test that out\n",
        "for s in ['Terve BERT! Hauskaa tavata!', 'Tuntematon: 你']:\n",
        "    print('Original string:', s)\n",
        "    print('Tokenized:', tokenizer.tokenize(s))\n",
        "    indices, segments = tokenizer.encode(s, max_len=20)    # max_len for padding and truncation\n",
        "    print('Encoded:', indices)\n",
        "    print('Segments:', segments)\n",
        "    print('Decoded:', ' '.join(tokenizer.decode(indices)))\n",
        "    print()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original string: Terve BERT! Hauskaa tavata!\n",
            "Tokenized: ['[CLS]', 'Terve', 'B', '##ER', '##T', '!', 'Hauskaa', 'tavata', '!', '[SEP]']\n",
            "Encoded: [102, 6309, 415, 9981, 50031, 380, 32676, 10594, 380, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded: Terve B ##ER ##T ! Hauskaa tavata !\n",
            "\n",
            "Original string: Tuntematon: 你\n",
            "Tokenized: ['[CLS]', 'Tunte', '##maton', ':', '你', '[SEP]']\n",
            "Encoded: [102, 26356, 4763, 236, 101, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded: Tunte ##maton : [UNK]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPM0l4mhAULY"
      },
      "source": [
        "## Vectorize data\n",
        "\n",
        "We'll use the familiar `LabelEncoder` for labels and the keras-bert `Tokenizer` for text data. `Y` is the representation of the labels that will be given to the model for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlrgdyKaAULZ",
        "outputId": "8ddad044-a356-4089-aadb-9cde545d87fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()    # Turns class labels into integers\n",
        "Y = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Take note of how many unique labels there are in the data\n",
        "num_labels = len(set(Y))\n",
        "\n",
        "\n",
        "# Print out some examples\n",
        "print('Number of unique labels:', num_labels)\n",
        "print(type(labels), labels[:10])\n",
        "print(type(Y), Y[:10])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique labels: 38\n",
            "<class 'list'> ['IN', 'IN', 'IN', 'IN', 'IG', 'NA', 'IN', 'NA', 'ID', 'SP']\n",
            "<class 'numpy.ndarray'> [12 12 12 12  8 21 12 21  4 37]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOCsh8h6AULe"
      },
      "source": [
        "Keep token indices and segment ids in separate lists and store as numpy arrays. `X` here is the final vectorized form of the input we'll be providing to the model for training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZUtvvdPAULf",
        "outputId": "a4d5ac9a-3bb0-4fec-9f36-5742370e0655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "token_indices, segment_ids = [], []\n",
        "for text in texts:\n",
        "    # tokenizer.encode() returns a sequence of token indices\n",
        "    # and a sequence of segment IDs. BERT expects both as input,\n",
        "    # even if the segments IDs are just all zeros (like here).\n",
        "    tid, sid = tokenizer.encode(text, max_len=INPUT_LENGTH)\n",
        "    token_indices.append(tid)\n",
        "    segment_ids.append(sid)\n",
        "\n",
        "# Format input as list of two numpy arrays\n",
        "X = [np.array(token_indices), np.array(segment_ids)]\n",
        "\n",
        "\n",
        "# Print some examples\n",
        "print('Token indices:')\n",
        "print(X[0][:2])\n",
        "print('Decoded:')\n",
        "for i in X[0][:2]:\n",
        "    print(tokenizer.decode(list(i)))\n",
        "print('Segment ids:')\n",
        "print(X[1][:2])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices:\n",
            "[[  102 16184   825 12725   145 16054  1037 11262 48314 28433 28813 28238\n",
            "    612 16054  1037  3677 43268   794   119   374   650   751  4107 17193\n",
            "   6719  3841   395   111 16184  3906   193   917   439  5827 35929 50006\n",
            "  11570 11832  1810   111  1643 10208  1300   333   105   193  1939  4747\n",
            "  40825 12046   142 13427  3677   166  6055 33193 46539  4332   145 41432\n",
            "  11946   492   142 37481  7063  1904 19529   111 32513 39220 26310  2831\n",
            "  26824 50045 16307 22389  7281  9064 12355 17448 24071 42365 16625 50098\n",
            "  50031 16184  3906 38708  4846   145  6299 38965 50009 21147  2686 32779\n",
            "   9615  1654 38348   612  3255   166 44140   111 16184  7504  7728  9506\n",
            "   1025 13522   142 10625 39401   119  8720   166   119 29236   166   119\n",
            "   4699   166   142 18779 11401  1732   119 38965 16410  9403 34353  5608\n",
            "    612 38965 50009  1124 47231   174  2799 29410   142  8302  3480   111\n",
            "    737 10568  7308   164 47431 16377  4689   142 16311  6597  8417 17178\n",
            "    119 10568  2304 15193  8483   142  1819   119   382  4689   142 16311\n",
            "    361  4614 36898 16472 11561 50016   111 16184  8087 10568 33559  8145\n",
            "   1940  3255 13426   507   103     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0]\n",
            " [  102  1320  1320  1320  4295 14392   120  1320  1320  1320   802  1320\n",
            "   1320  1320  6613   949 50006 29885 50008  1320  1320  1320   348  1320\n",
            "   1320  9104 14576   265 50011   384   249  1320  1320   308   145  2299\n",
            "   3176  1484  4059   256 14078   203  2296   348  1320  1320 42033  1320\n",
            "   1320   308 19086 17327 43417  1000  7042   111 48243   145 12094  1883\n",
            "  25051  1800  3053 17839  9547  1200   111  1713 15199  1688   407  4557\n",
            "  14392 15469  1534  7589   607  1530 12390  8097  9523 30438   418  9139\n",
            "    111  4295 14392   120 12819 20963 26522  5067 22894   109 18163  8467\n",
            "   6502   461   111 48243  3267  7571 42971  1956  3043  3043 49386   101\n",
            "    263  7122   333  3013  3013 10764   142  2401  3915  2805  7704  3267\n",
            "   1118   341  1066 29554   111  2177  1320  1320  1320  4295 14392   120\n",
            "   1320  1320  1320   802  1320  1320  1320  6613   949 50006 29885 50008\n",
            "   1320  1320  1320   348  1320  1320  9104 14576   265 50011   384   249\n",
            "   1320  1320   308   145  2299  3176  1484  4059   256 14078   203  2296\n",
            "    348  1320  1320 42033  1320  1320   308 19086 17327 43417  1000  7042\n",
            "    111 48243   145 12094  1883  7174 50006  3053   435  9547  1200   111\n",
            "   1713 15199  1688   407  4557 14392 15469  1534  7589   607  1530 12390\n",
            "   8097   103     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0]]\n",
            "Decoded:\n",
            "['Harjoit', '##telu', 'Alla', 'on', 'Psy', '##lin', 'opiskelijat', '##oimikunnan', 'määritelmä', 'harjoittelun', 'tavoitteista', 'sekä', 'Psy', '##lin', 'palkka', '##suos', '##itus', ',', 'joka', 'tulee', 'pitää', 'mielessä', 'harjoittelu', '##paikkaa', 'etsi', '##essä', '.', 'Harjoit', '##telun', 'ei', 'tule', 'olla', 'huonosti', 'palkattu', '##a', 'saati', 'palkat', '##onta', '.', 'Pal', '##kkojen', 'pol', '##jen', '##ta', 'ei', 'aja', 'minkään', 'tahon', 'etuja', 'ja', 'vastaavasti', 'palkka', '-', 'asioiden', 'hoitaminen', 'suositusten', 'mukaisesti', 'on', 'signaali', 'kestävä', '##stä', 'ja', 'ammattitaitoa', 'arvosta', '##vasta', 'kehityksestä', '.', 'HA', '##RJ', '##OI', '##TT', '##ELU', '##N', 'TA', '##VO', '##IT', '##TE', '##ET', 'JA', 'SI', '##SÄ', '##LL', '##Ö', '##T', 'Harjoit', '##telun', 'keskeisenä', 'tavoitteena', 'on', 'kehittää', 'psykologi', '##n', 'työtehtä', '##vissä', 'tarvittavia', 'ammati', '##llisia', 'valmiuksia', 'sekä', 'ammatti', '-', 'identiteettiä', '.', 'Harjoit', '##telussa', 'opiskelija', 'pereh', '##tyy', 'yksilön', 'ja', 'yhteisön', 'arviointiin', ',', 'ohjaus', '-', ',', 'neuvonta', '-', ',', 'hoito', '-', 'ja', 'kuntoutus', '##menetel', '##miin', ',', 'psykologi', '##sten', 'palvelujen', 'järjestelmien', 'toimintaan', 'sekä', 'psykologi', '##n', 'työn', 'eettis', '##iin', 'lähtö', '##kohtiin', 'ja', 'velvoit', '##teisiin', '.', 'Hän', 'oppii', 'käyttämään', 'jo', 'omaksu', '##miaan', 'tietoja', 'ja', 'taitoja', 'käytännön', 'työt', '##ilanteessa', ',', 'oppii', 'uusia', 'työta', '##itoja', 'ja', 'tietää', ',', 'mitä', 'tietoja', 'ja', 'taitoja', 'hän', 'tarvitsee', 'voidakseen', 'kehittyä', 'työntekijän', '##ä', '.', 'Harjoit', '##telija', 'oppii', 'työskentelemään', 'yhteistyössä', 'muiden', 'ammatti', '##ryhmien', 'kanssa']\n",
            "[\"'\", \"'\", \"'\", 'Man', '##let', '##it', \"'\", \"'\", \"'\", 'eli', \"'\", \"'\", \"'\", 'pienet', 'ihmis', '##a', '##pina', '##t', \"'\", \"'\", \"'\", '(', \"'\", \"'\", 'homo', 'sap', '##ien', '##s', 'min', '##or', \"'\", \"'\", ')', 'on', 'pieni', '##koko', '##isista', 'yksilö', '##istä', 'muodostu', '##va', 'ihmisten', '(', \"'\", \"'\", 'Homo', \"'\", \"'\", ')', 'suvun', 'muta', '##nt', '##tila', '##ji', '.', 'Laji', 'on', 'kehittynyt', 'Euroopan', 'bu', '##ona', '##par', '##teni', '##hm', '##isestä', '.', 'Kar', '##keasti', 'arvo', '##iden', 'man', '##let', '##eiksi', 'voidaan', 'laskea', 'kaikki', 'alle', '190', '##cm', 'pitkät', 'miespuol', '##iset', 'henkilöt', '.', 'Man', '##let', '##it', 'pyrkivät', 'tyypillisesti', 'kompens', '##oimaan', 'pituutta', '##an', 'materiaali', '##silla', 'tarpe', '##illa', '.', 'Laji', 'elää', 'pääasiassa', 'lois', '##ena', '[', '[', 'alfa', '[UNK]', 'al', '##fo', '##jen', ']', ']', 'rinnalla', 'ja', 'vaati', 'huomattavasti', 'vähemmän', 'energiaa', 'elää', '##kseen', 'kuin', 'nyky', '##ihminen', '.', '+', \"'\", \"'\", \"'\", 'Man', '##let', '##it', \"'\", \"'\", \"'\", 'eli', \"'\", \"'\", \"'\", 'pienet', 'ihmis', '##a', '##pina', '##t', \"'\", \"'\", \"'\", '(', \"'\", \"'\", 'homo', 'sap', '##ien', '##s', 'min', '##or', \"'\", \"'\", ')', 'on', 'pieni', '##koko', '##isista', 'yksilö', '##istä', 'muodostu', '##va', 'ihmisten', '(', \"'\", \"'\", 'Homo', \"'\", \"'\", ')', 'suvun', 'muta', '##nt', '##tila', '##ji', '.', 'Laji', 'on', 'kehittynyt', 'Euroopan', 'bon', '##a', '##par', '##tei', '##hm', '##isestä', '.', 'Kar', '##keasti', 'arvo', '##iden', 'man', '##let', '##eiksi', 'voidaan', 'laskea', 'kaikki', 'alle', '190', '##cm']\n",
            "Segment ids:\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX8eBfWJAULi"
      },
      "source": [
        "## Load pretrained BERT model\n",
        "\n",
        "We'll use the keras-bert function `load_trained_model_from_checkpoint` to load the model from the checkpoint we downloaded earlier.\n",
        "\n",
        "Explanation for a few parameters from keras-bert documentation:\n",
        "\n",
        "* `training`: If `training`, the whole model will be returned. Otherwise, the MLM [masked language modeling] and NSP [next sentence prediction] parts will be ignored.\n",
        "* `trainable`: Whether the model is trainable. The default value is the same with `training`.\n",
        "\n",
        "We don't need the masked language modeling or next sentence prediction parts (these are primarily for pretraining), so we'll use `training=False` but `trainable=True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lCTrDBjAULj",
        "outputId": "5798db65-9275-45e9-a043-ae1ceee59efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "\n",
        "\n",
        "pretrained_model = load_trained_model_from_checkpoint(\n",
        "    config_file = bert_config_path,\n",
        "    checkpoint_file = bert_checkpoint_path,\n",
        "    training = False,\n",
        "    trainable = True,\n",
        "    seq_len = INPUT_LENGTH\n",
        ")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fac24c29a58>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fac24c29a58>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fac24c23860>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fac24c23860>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac24c26b70>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac24c26b70>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac24bb0390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac24bb0390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac24b5ec50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac24b5ec50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac24b87be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac24b87be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac24b760b8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac24b760b8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac24a9a048>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac24a9a048>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac24a90a58>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac24a90a58>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac24ace9e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac24ace9e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fae525ea5f8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fae525ea5f8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fae525ea390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fae525ea390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fae526207b8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fae526207b8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac24a5f438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac24a5f438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac249ea908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac249ea908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac249da5c0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac249da5c0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac24a214a8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac24a214a8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac248ffba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac248ffba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac248ff4a8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac248ff4a8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac248a6f60>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac248a6f60>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac2491f400>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac2491f400>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac248d9fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac248d9fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac2487ee48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac2487ee48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac247b96d8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac247b96d8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac24768710>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac24768710>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac247d3ef0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac247d3ef0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac247d39e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac247d39e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac246ab0b8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac246ab0b8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac246a0358>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac246a0358>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac245ace10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac245ace10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac246974e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac246974e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac2465e278>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac2465e278>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac245fa0b8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac245fa0b8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac245eeac8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac245eeac8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac24477a90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac24477a90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac245109e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac245109e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac24521ef0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac24521ef0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac244119e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac244119e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac244c5278>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac244c5278>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac2433d4e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac2433d4e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac244b7be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac244b7be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac243e7fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac243e7fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac242d9c88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac242d9c88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac242e2080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac242e2080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac242036d8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac242036d8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac241f6400>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac241f6400>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac241f19b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac241f19b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac24251978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac24251978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac24251b00>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac24251b00>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac240cdcc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac240cdcc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac2412deb8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac2412deb8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac240f8f60>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac240f8f60>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac240b89e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac240b89e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac24060da0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac24060da0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac23f94e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac23f94e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac23fed908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac23fed908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac23fb9e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac23fb9e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac23fdcfd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac23fdcfd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac23fd14a8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac23fd14a8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac23e589b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac23e589b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac23e4b5c0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac23e4b5c0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac23deb5f8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac23deb5f8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac23e9b5c0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac23e9b5c0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN4TGMOFAULm"
      },
      "source": [
        "Let's have a bit of a look at that model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFG5cmMCAULm",
        "outputId": "80e95118-1854-4372-8913-707af7eb21e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# This is a keras model, so we can figure out what inputs it takes like so:\n",
        "pretrained_model.inputs"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'Input-Token_2:0' shape=(?, 250) dtype=float32>,\n",
              " <tf.Tensor 'Input-Segment_2:0' shape=(?, 250) dtype=float32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPZUpN5BAULq",
        "outputId": "96d13576-52c7-4ec9-ef7f-2cdd2e8c2a52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# And similarly for outputs:\n",
        "pretrained_model.outputs"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'Encoder-12-FeedForward-Norm_2/add_1:0' shape=(?, 250, 768) dtype=float32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RVtT5NUAULx"
      },
      "source": [
        "So, the model takes `Input-Token` and `Input-Segment` inputs, both of dimension (batch-size, input-length), and produces a single output tensor of dimension (batch-size, input-length, hidden-dim). The input matches our `X`, but we'll need to work on the output a bit as our `Y` is just a label for each input, not a sequence of labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3u_f9tTAULy",
        "outputId": "f27ae7a0-ce46-4316-fb23-5aacced0b532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pretrained_model.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, 250)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, 250)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 250, 768), ( 38480640    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 250, 768)     1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 250, 768)     0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 250, 768)     192000      Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 250, 768)     0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 250, 768)     1536        Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 250, 768)     2362368     Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 250, 768)     0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 250, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 250, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 250, 768)     0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 250, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 250, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 250, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 250, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 250, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 250, 768)     0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 250, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 250, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 250, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 250, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, 250, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, 250, 768)     0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, 250, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, 250, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 250, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 250, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, 250, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, 250, 768)     0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, 250, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, 250, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 250, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 250, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, 250, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, 250, 768)     0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, 250, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, 250, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 250, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 250, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, 250, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, 250, 768)     0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, 250, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, 250, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 250, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 250, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, 250, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, 250, 768)     0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, 250, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, 250, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 250, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 250, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, 250, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, 250, 768)     0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, 250, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, 250, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 250, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 250, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 250, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, 250, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, 250, 768)     0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, 250, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, 250, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 250, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 250, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 250, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 250, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, 250, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, 250, 768)     0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, 250, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, 250, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 250, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 250, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 250, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 250, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, 250, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, 250, 768)     0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, 250, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, 250, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 250, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 250, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 250, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 250, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, 250, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, 250, 768)     0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, 250, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, 250, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
            "==================================================================================================\n",
            "Total params: 123,730,176\n",
            "Trainable params: 123,730,176\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E30Y1SzAUL1"
      },
      "source": [
        "This is a regular Keras model. In Keras, models behave very much like layers, so we're able to wrap this in our own model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhanxtTeAUL1"
      },
      "source": [
        "## Build classification model\n",
        "\n",
        "We'll make a *very* simple model for text classification: just attach a dense layer to the output for the special `[CLS]` token, and connect the model inputs to the BERT model inputs.\n",
        "\n",
        "Recall that in the BERT input representation, each sequence starts with the special `[CLS]` token and the corresponding output is used in BERT pretraining for the next sentence prediction task. We'll use the `[CLS]` output similarly for our classification task.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/TurkuNLP/Deep_Learning_in_LangTech_course/master/bert-representation.png\" style=\"width: 80%\">\n",
        "\n",
        "(Figure from [Devlin et al.](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "First, let's find the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfK7MqUaAUL2",
        "outputId": "061a9994-6727-4be4-e836-9ad18ac4ffce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(pretrained_model.outputs)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'Encoder-12-FeedForward-Norm_2/add_1:0' shape=(?, 250, 768) dtype=float32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGO0TcwOAUL8"
      },
      "source": [
        "Those dimensions are (minibatch-size, sequence-length, hidden-dim).\n",
        "\n",
        "We'll just need the first sequence position across all elements in the initial (minibatch) dimension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUBL8s-DAUL9",
        "outputId": "9d5fc0d8-ca5a-4946-94ac-9ffc693183c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# model.outputs is a list, here with a single item. Here\n",
        "# pretrained_model.outputs[0] just grabs that item (the output tensor).\n",
        "# Indexing that tensor with [:,0] gives the first position in the sequence\n",
        "# for all elements in the batch (the `:`).\n",
        "bert_out = pretrained_model.outputs[0][:,0]\n",
        "\n",
        "print(bert_out)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"strided_slice_2:0\", shape=(?, 768), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQlWQRjNAUMB"
      },
      "source": [
        "Then we can simply create our model. This is just basic Keras, where the pretrained BERT model is behaving essentially as a layer of our \"wrapping\" model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIIwrxfRAUMC"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "out = Dense(num_labels, activation='softmax')(bert_out)\n",
        "model = Model(\n",
        "    inputs=pretrained_model.inputs,\n",
        "    outputs=[out]\n",
        ")"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXOxbnDWAUMF"
      },
      "source": [
        "## Create optimizer\n",
        "\n",
        "BERT is pretrained with an Adam optimizer with warmup and regularization using weight decay. We won't go into detail on these optimizer settings, but will instead largely copy parameters used in the original BERT work.\n",
        "\n",
        "(If you're interested in tuning the training process, trying different values of `LEARNING_RATE` is a good place to start!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgK4eUfWAUMF"
      },
      "source": [
        "from keras_bert import calc_train_steps, AdamWarmup\n",
        "\n",
        "\n",
        "# Calculate the number of steps for warmup\n",
        "total_steps, warmup_steps = calc_train_steps(\n",
        "    num_example=len(texts),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    warmup_proportion=0.1,\n",
        ")\n",
        "\n",
        "optimizer = AdamWarmup(\n",
        "    total_steps,\n",
        "    warmup_steps,\n",
        "    lr=LEARNING_RATE,\n",
        "    epsilon=1e-6,\n",
        "    weight_decay=0.01,\n",
        "    weight_decay_pattern=['embeddings', 'kernel', 'W1', 'W2', 'Wk', 'Wq', 'Wv', 'Wo']\n",
        ")"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7Urz1LrAUMI"
      },
      "source": [
        "## Train model\n",
        "\n",
        "The model is compiled and trained normally. As usual, we'll use `sparse_categorical_crossentropy` loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTLUbIS2AUMJ"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['sparse_categorical_accuracy']\n",
        ")"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm0pG3aaAUMM"
      },
      "source": [
        "Training, as usual. (Note: this will take a fair bit of time unless you're running with GPU acceleration.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnDDZYJGAUMM",
        "outputId": "dc585d99-b0a9-4b49-e999-adf691a73edf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "history = model.fit(\n",
        "    X,\n",
        "    Y,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.1\n",
        ")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4500 samples, validate on 500 samples\n",
            "Epoch 1/3\n",
            "4500/4500 [==============================] - 350s 78ms/sample - loss: 1.3663 - sparse_categorical_accuracy: 0.6280 - val_loss: 0.9674 - val_sparse_categorical_accuracy: 0.7220\n",
            "Epoch 2/3\n",
            "4500/4500 [==============================] - 343s 76ms/sample - loss: 0.6605 - sparse_categorical_accuracy: 0.8040 - val_loss: 0.8256 - val_sparse_categorical_accuracy: 0.7460\n",
            "Epoch 3/3\n",
            "4500/4500 [==============================] - 345s 77ms/sample - loss: 0.3306 - sparse_categorical_accuracy: 0.9080 - val_loss: 0.8859 - val_sparse_categorical_accuracy: 0.7520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IkBs1QxAUMT"
      },
      "source": [
        "Let's plot that training history:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9RAvzLIAUMU",
        "outputId": "9b5f0172-8482-41f4-ab08-6db43555df4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.plot(history.history['sparse_categorical_accuracy'],label=\"Training set accuracy\")\n",
        "    plt.plot(history.history['val_sparse_categorical_accuracy'],label=\"Validation set accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuHjb7uFAUMX"
      },
      "source": [
        "This is definitely starting to overfit by the third epoch, but the best dev set result is well in line with the usual results on this data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ftqm1TyAUMX"
      },
      "source": [
        "## Predict with the trained model\n",
        "\n",
        "Let's run a few simple cases through the trained model to illustrate its use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e8HUgqBAUMY",
        "outputId": "705bf997-3990-4525-e783-9a77effef0e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import numpy\n",
        "\n",
        "def predict_one(text):\n",
        "    # This function takes a string, vectorizes it with the tokenizer,\n",
        "    # and returns the model prediction. The vectorization follows\n",
        "    # the pattern we saw above for training data preparation:\n",
        "    tid, sid = tokenizer.encode(text, max_len=INPUT_LENGTH)\n",
        "    test_token_indices = [tid]\n",
        "    test_segment_ids = [sid]\n",
        "    test_X = [np.array(test_token_indices), np.array(test_segment_ids)]\n",
        "    # Run model.predict for this single case \n",
        "    predictions = model.predict(test_X)\n",
        "    # model.predict always returns a sequence. As we there's only a single input,\n",
        "    # we'll just return the value of the first (and only) element.\n",
        "    return predictions[0]\n",
        "\n",
        "\n",
        "# Test the model with a few strings of our own.\n",
        "test_strings = [\n",
        "  'Tämä on todella surkea elokuva!',\n",
        "  'Vuonna 1939 alkoi talvisota, se kesti muutaman kuukauden.'\n",
        "]\n",
        "\n",
        "predictions=[]\n",
        "for s in test_strings:\n",
        "    predictions.append(numpy.argmax(predict_one(s)))\n",
        "print(label_encoder.inverse_transform(predictions))\n",
        "    "
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['OP' 'NA']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}